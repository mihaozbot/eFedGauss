{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miha\\AppData\\Local\\Temp\\ipykernel_3344\\3119565666.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from utils.utils_train import train_supervised, train_models_in_threads, test_model_in_batches\n",
    "from utils.utils_plots import plot_interesting_features, plot_metric_data, save_figure, plot_cluster_data\n",
    "from utils.utils_dataset import prepare_dataset, balance_data_for_clients\n",
    "from utils.utils_dataset import display_dataset_split\n",
    "from utils.utils_metrics import calculate_metrics, plot_confusion_matrix, calculate_roc_auc\n",
    "import threading\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.eFedGauss import eFedGauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1           58.862440\n",
      "V2           94.773457\n",
      "V3           57.708148\n",
      "V4           22.558515\n",
      "V5          148.544973\n",
      "V6           99.462131\n",
      "V7          164.146736\n",
      "V8           93.223927\n",
      "V9           29.029061\n",
      "V10          48.333399\n",
      "V11          16.816387\n",
      "V12          26.532107\n",
      "V13          12.918764\n",
      "V14          29.741092\n",
      "V15          13.376686\n",
      "V16          31.444966\n",
      "V17          34.416326\n",
      "V18          14.539815\n",
      "V19          12.805499\n",
      "V20          93.918625\n",
      "V21          62.033221\n",
      "V22          21.436234\n",
      "V23          67.336147\n",
      "V24           7.421176\n",
      "V25          17.814986\n",
      "V26           6.121896\n",
      "V27          54.177877\n",
      "V28          49.277892\n",
      "Amount    25691.160000\n",
      "Class         1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = 'creditcard.csv' #Download from https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "data = pd.read_csv(file_path)\n",
    "feature_dim = 29\n",
    "\n",
    "# Remove the first dimension/column\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "\n",
    "# Compute the ranges\n",
    "ranges = data.max() - data.min()\n",
    "\n",
    "# Display the ranges\n",
    "print(ranges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(f\"{torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #torch.device(\"cpu\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment to determine best problem-specific meta-parameters\n",
    "flag_settings_experiment = 0\n",
    "if flag_settings_experiment:\n",
    "\n",
    "    num_clients = 1\n",
    "\n",
    "    # Define the range of values for each parameter\n",
    "    num_sigma_values = [5, 10, 15, 20]\n",
    "    kappa_join_values = [0.3, 0.5, 0.8]\n",
    "    N_r_values = [10, 20, 30]\n",
    "\n",
    "    # Total number of experiments\n",
    "    total_experiments = len(num_sigma_values) * len(kappa_join_values) * len(N_r_values)\n",
    "    completed_experiments = 0\n",
    "\n",
    "    # Define other parameters and data setup\n",
    "    local_model_params = {\n",
    "        \"feature_dim\": feature_dim,\n",
    "        \"num_classes\": 2,\n",
    "        \"kappa_n\": 1,\n",
    "        \"S_0\": 1e-10,\n",
    "        \"c_max\": 100,\n",
    "        \"device\": device\n",
    "    }\n",
    "\n",
    "    # Placeholder for the best parameters and best score\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "\n",
    "    # List to store all results\n",
    "    results = []\n",
    "\n",
    "    # Function to write data to a file\n",
    "    def write_to_file(file_path, data, mode='a'):\n",
    "        with open(file_path, mode) as file:\n",
    "            file.write(data + \"\\n\")\n",
    "\n",
    "    # Prepare the dataset\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    client_train_unbalanced, test_data, all_data = prepare_dataset(X, y, num_clients)\n",
    "    client_train = balance_data_for_clients(client_raw_data=client_train_unbalanced, balance=[\"random\"], local_models=None, round=1)\n",
    "    \n",
    "    # Initialize a lock and a shared variable for progress tracking\n",
    "    lock = threading.Lock()\n",
    "    completed_experiments = 0\n",
    "    total_experiments = len(num_sigma_values) * len(kappa_join_values) * len(N_r_values)\n",
    "\n",
    "    # Function to execute model training and evaluation\n",
    "    def train_evaluate_model(params):\n",
    "        global completed_experiments\n",
    "        \n",
    "        num_sigma, kappa_join, N_r = params\n",
    "        local_model_params.update({\"num_sigma\": num_sigma, \"kappa_join\": kappa_join, \"N_r\": N_r})\n",
    "\n",
    "        local_model = eFedGauss(**local_model_params)\n",
    "        train_supervised(local_model, client_train[0])\n",
    "\n",
    "        _, pred_max, _ = test_model_in_batches(local_model, test_data, batch_size = 1000)\n",
    "        metrics = calculate_metrics(pred_max, test_data, weight=\"binary\")\n",
    "\n",
    "        result_str = f\"num_sigma:{num_sigma}, kappa_join:{kappa_join}, N_r:{N_r}, f1_score:{metrics['f1_score']}, precission:{metrics['precision']}, recall:{metrics['recall']}\"\n",
    "        print(result_str)\n",
    "        write_to_file(\"experiment_results.txt\", result_str)  # Write results to file\n",
    "        \n",
    "        # Update progress\n",
    "        with lock:\n",
    "            completed_experiments += 1\n",
    "            progress = (completed_experiments / total_experiments) * 100\n",
    "            print(f\"Progress: {completed_experiments}/{total_experiments} ({progress:.1f}%)\")\n",
    "\n",
    "        return {\"num_sigma\": num_sigma, \"kappa_join\": kappa_join, \"N_r\": N_r, \"f1_score\": metrics[\"f1_score\"], \"precission\": metrics[\"precision\"], \"recall\": metrics[\"recall\"]}\n",
    "        \n",
    "\n",
    "    # Write initial setup data to file\n",
    "    initial_setup_str = f\"Initial Setup: num_clients={num_clients}, num_sigma_values={num_sigma_values}, kappa_join_values={kappa_join_values}, N_r_values={N_r_values}\"\n",
    "    write_to_file(\"experiment_results.txt\", initial_setup_str, mode='w')  # 'w' to overwrite if exists\n",
    "\n",
    "    # Using ThreadPoolExecutor to run in multiple threads\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        param_combinations = list(itertools.product(num_sigma_values, kappa_join_values, N_r_values))\n",
    "        results = list(executor.map(train_evaluate_model, param_combinations))\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model1, model2):\n",
    "    differences = []\n",
    "\n",
    "    # Function to find differing indices within the overlapping range\n",
    "    def find_differing_indices(tensor1, tensor2):\n",
    "        min_length = min(tensor1.size(0), tensor2.size(0))\n",
    "        differing = (tensor1[:min_length] != tensor2[:min_length]).nonzero(as_tuple=False)\n",
    "        if differing.nelement() == 0:\n",
    "            return \"No differences\"\n",
    "        else:\n",
    "            return differing.view(-1).tolist()  # Flatten and convert to list\n",
    "\n",
    "    # Compare mu parameter and find differing indices\n",
    "    mu_equal = torch.equal(model1.mu[:model1.c], model2.mu[:model2.c])\n",
    "    if not mu_equal:\n",
    "        differing_indices_mu = find_differing_indices(model1.mu[:model1.c], model2.mu[:model2.c])\n",
    "        differences.append(f\"mu parameter differs at indices {differing_indices_mu}\")\n",
    "\n",
    "    # Compare S parameter and find differing indices\n",
    "    S_equal = torch.equal(model1.S[:model1.c], model2.S[:model2.c])\n",
    "    if not S_equal:\n",
    "        differing_indices_S = find_differing_indices(model1.S[:model1.c], model2.S[:model2.c])\n",
    "        differences.append(f\"S parameter differs at indices {differing_indices_S}\")\n",
    "\n",
    "    # Compare n parameter and find differing indices\n",
    "    n_equal = torch.equal(model1.n[:model1.c], model2.n[:model2.c])\n",
    "    if not n_equal:\n",
    "        differing_indices_n = find_differing_indices(model1.n[:model1.c], model2.n[:model2.c])\n",
    "        differences.append(f\"n parameter differs at indices {differing_indices_n}\")\n",
    "\n",
    "    # Check if there are any differences\n",
    "    if differences:\n",
    "        difference_str = \", \".join(differences)\n",
    "        return False, f\"Differences found in: {difference_str}\"\n",
    "    else:\n",
    "        return True, \"Models are identical\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for saving the experiments\n",
    "def write_to_file(file_path, data, mode='a'):\n",
    "    with open(file_path, mode) as file:\n",
    "        file.write(data + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function handles the learning of the federated model, one experiment\n",
    "def run_individual_experiment(federated_model_params, local_model_params, num_clients, num_rounds, client_raw_train, test_data, balance_techniques, test_clients=True):\n",
    "\n",
    "    # Initialize a model for each client\n",
    "    local_models = [eFedGauss(**local_model_params) for _ in range(num_clients)]\n",
    "    federated_model = eFedGauss(**federated_model_params)\n",
    "\n",
    "    # Initialize a list to store the metrics for each round\n",
    "    round_metrics = []\n",
    "    client_train = []\n",
    "\n",
    "    result_file = \"experiment_results.txt\"\n",
    "  \n",
    "    # Execute communication loop\n",
    "    for round in range(num_rounds):\n",
    "\n",
    "        start = time.time() #For debugging\n",
    "\n",
    "        print(f\"\\n --- \\n --- Communication Round {round + 1} ---\")\n",
    "        round_info = f\"--- Communication Round {round + 1} ---\\n---\\n\"\n",
    "        \n",
    "        # Undersample client data in each round \n",
    "        client_train = balance_data_for_clients(client_raw_data=client_raw_train, balance_techniques=balance_techniques, local_models=local_models, round=round)\n",
    "        display_dataset_split(client_train, test_data)\n",
    "\n",
    "        # Train local models in parallel threads\n",
    "        train_models_in_threads(local_models, client_train)\n",
    "\n",
    "        # Update federated model with local models\n",
    "        for client_idx, _ in enumerate(local_models):\n",
    "\n",
    "            print(f\"\\n Updating agreggated model with client {client_idx + 1}\")\n",
    "            print(f\"Number of local model clusters = {sum(local_models[client_idx].n[0:local_models[client_idx].c] > 0)}\")\n",
    "            local_models[client_idx].federal_agent.federated_merging()\n",
    "            print(f\"Number of local model clusters after merging = {sum(local_models[client_idx].n[0:local_models[client_idx].c] > 0)}\")\n",
    "\n",
    "            # Calculate and collect metrics for each client model\n",
    "            # The testing was moved further, to after the federated model is returned to the clients\n",
    "            #_, client_pred, _ = test_model_in_batches(local_models[client_idx], client_train[client_idx], batch_size=500)\n",
    "            #client_binary = calculate_metrics(client_pred, client_train[client_idx], \"binary\")\n",
    "            #print(f\"Train Metrics: {client_binary}\")\n",
    "            #plot_confusion_matrix(pred_max, clients_data[client_idx])\n",
    "\n",
    "\n",
    "            federated_model.federal_agent.merge_model_privately(local_models[client_idx], local_models[client_idx].kappa_n, pred_min = 0)\n",
    "            federated_model.federal_agent.federated_merging()\n",
    "            \n",
    "            print(f\"Federated clusters after merging = {sum(federated_model.n[0:federated_model.c]> federated_model.kappa_n)}\")\n",
    "                \n",
    "        print(\"\\n\")\n",
    "    \n",
    "        # Evaluate federated model\n",
    "        fed_scores, fed_pred, _ = test_model_in_batches(federated_model, test_data, batch_size=1000)\n",
    "        fed_binary = calculate_metrics(fed_pred, test_data, \"binary\")\n",
    "        fed_roc_auc = calculate_roc_auc(fed_scores, test_data)\n",
    "        print(f\"Test Metrics: {fed_binary}\")\n",
    "        print(f\"Test ROC AUC: {fed_roc_auc}\")\n",
    "        #plot_confusion_matrix(pred_max_fed, test_data) #Mainly for debugging\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Return the updated federated model to each client\n",
    "        client_metrics = [] # Reset client metrics for the new round\n",
    "        for client_idx, _ in enumerate(local_models):\n",
    "            print(f\"Returning updated model to client {client_idx + 1}\")\n",
    "            \n",
    "            # Normalize the covariance matrices and reset the number of samples\n",
    "            # This is required as the number of samples increases exponentially with communication\n",
    "            with torch.no_grad():    \n",
    "                local_models[client_idx].S.data /= torch.min(local_models[client_idx].n[:local_models[client_idx].c].data.clone())\n",
    "                local_models[client_idx].n.data /= torch.min(local_models[client_idx].n[:local_models[client_idx].c].data.clone())\n",
    "                local_models[client_idx].S_glo.data /= torch.sum(local_models[client_idx].n_glo).unsqueeze(-1)\n",
    "                local_models[client_idx].n_glo = torch.ones_like(local_models[client_idx].n_glo)\n",
    "\n",
    "            #Return federated model back to the clients\n",
    "            local_models[client_idx].federal_agent.merge_model_privately(federated_model, 0, pred_min = 0)\n",
    "            #local_models[client_idx].federal_agent.federated_merging() #This could be useful but it is somewhat redundant\n",
    "            print(f\"Number of local model clusters after transfer = {sum(local_models[client_idx].n[0:local_models[client_idx].c]> 0)}\")\n",
    "\n",
    "            # Testing the clients also, for debugging and illustration plot\n",
    "            if test_clients:\n",
    "\n",
    "                # Calculate and collect metrics for each client model\n",
    "                _, client_pred, _ = test_model_in_batches(local_models[client_idx], test_data, batch_size=500)\n",
    "                client_binary = calculate_metrics(client_pred, test_data, \"binary\")\n",
    "                print(f\"Test Metrics client {client_idx} after merge: {client_binary}\")\n",
    "                # plot_confusion_matrix(pred_max, clients_data[client_idx])\n",
    "                \n",
    "                # Calculate additional metrics for each client\n",
    "                client_metrics.append({\n",
    "                    'client_idx': client_idx,\n",
    "                    'binary': client_binary,\n",
    "                    'clusters': sum(local_models[client_idx].n[0:local_models[client_idx].c].cpu()> 0)\n",
    "                })\n",
    "            \n",
    "            #local_models[client_idx].score = torch.ones_like(local_models[client_idx].score)\n",
    "            local_models[client_idx].num_pred = torch.ones_like(local_models[client_idx].num_pred)\n",
    "            local_models[client_idx].age = torch.ones_like(local_models[client_idx].age)\n",
    "\n",
    "        # Reset the number of samples per cluster\n",
    "        # The number of clusters increases exponentially if this is not handled\n",
    "        with torch.no_grad():     \n",
    "            federated_model.S[:federated_model.c].data /= torch.min(federated_model.n[:federated_model.c].data.clone())\n",
    "            federated_model.n[:federated_model.c].data /= torch.min(federated_model.n[:federated_model.c].data.clone())\n",
    "            federated_model.S_glo.data /= torch.sum(federated_model.n_glo).unsqueeze(-1)\n",
    "            federated_model.n_glo = torch.ones_like(federated_model.n_glo)\n",
    "        \n",
    "        #federated_model.num_pred = torch.ones_like(federated_model.num_pred) #Future work, evaluation of accuracy\n",
    "\n",
    "        # Print and write round information to file\n",
    "        round_info = f\"--- End of Round {round + 1} ---\\n\"\n",
    "        print(round_info)\n",
    "\n",
    "        # Save the results of this round of communication\n",
    "        round_metrics.append({\n",
    "            'round': round + 1,\n",
    "            'federated_model': {\n",
    "                'clusters': sum(federated_model.n[0:federated_model.c].cpu() > federated_model.kappa_n),\n",
    "                'binary': fed_binary,\n",
    "                'roc_auc': fed_roc_auc\n",
    "            },\n",
    "            'client_metrics': client_metrics\n",
    "        })\n",
    "        \n",
    "        # Plot features for the current round\n",
    "        plt.close('all')  # Close all existing plots to free up memory\n",
    "        if 0:\n",
    "            #fig1 = plot_interesting_features(client_train[0], model=federated_model, num_sigma=federated_model.num_sigma, N_max=federated_model.kappa_n)   \n",
    "            #save_figure(fig1, \"./Images/credit_fraud_clusters\", format='pdf')\n",
    "            fig2 = plot_interesting_features(client_train[0], model=federated_model, num_sigma=2, N_max=federated_model.kappa_n)   \n",
    "            save_figure(fig2, \".Images/credit_fraud_samples.pdf\", format='pdf')\n",
    "\n",
    "        # Iterate over each round's metrics and write to file\n",
    "        for metric in round_metrics:\n",
    "            metric_info = f\"Round {metric['round']}: Metrics: {metric['federated_model']['binary']}, ROC AUC: {metric['federated_model']['roc_auc']}\\n\"\n",
    "            print(metric_info)  # Print each round's metrics\n",
    "            try:\n",
    "                write_to_file(result_file, metric_info)  # Write to file, for debugging\n",
    "            except:\n",
    "                print(\"Could not write to file.\")\n",
    "                pass\n",
    "        \n",
    "        # Save the experiment intermediate results for progress checking and debugging\n",
    "        experiment_file = f\".Results/mid_experiment_{federated_model_params['num_sigma']}_{int(federated_model_params['kappa_join']*10)}_{federated_model_params['N_r']}_{num_clients}_{len(balance_techniques)}_{federated_model_params['c_max']}.pth\"\n",
    "        torch.save(round_metrics, experiment_file)\n",
    "        print(f\"Saved experiment to {experiment_file}\")\n",
    "        print(f\"Round time was {(time.time() - start):.1f}s\")\n",
    "\n",
    "    # After all rounds print and save the metrics\n",
    "    final_info = \"All Rounds Completed. Metrics Collected:\\n\"\n",
    "    print(final_info)\n",
    "\n",
    "    # Iterate over each round's metrics and write to file\n",
    "    for metric in round_metrics:\n",
    "        metric_info = f\"Round {metric['round']}: \"\n",
    "        metric_info += f\"Federated Model - Clusters: {metric['federated_model']['clusters']}, \"\n",
    "        metric_info += f\"Binary Metrics: {metric['federated_model']['binary']}, ROC AUC: {metric['federated_model']['roc_auc']}\\n\"\n",
    "\n",
    "        for client_metric in metric['client_metrics']:\n",
    "            metric_info += f\"Client {client_metric['client_idx']} - Binary: {client_metric['binary']}\\n\"\n",
    "\n",
    "        print(metric_info)  # Print each round's metrics\n",
    "        #write_to_file(result_file, metric_info)  # Write to file\n",
    "\n",
    "    return round_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment to determine best settings for furhter experimentation\n",
    "# Define the range of values for each parameter\n",
    "num_sigma_values = [10, 15, 20]\n",
    "kappa_join_values = [0.3, 0.4, 0.5, 0.8, 1, 2]\n",
    "N_r_values = [15, 20, 25, 30]\n",
    "proportion = [5, 3, 1]\n",
    "\n",
    "# List of client counts and data configuration indices\n",
    "client_counts = [3]\n",
    "\n",
    "# Number of communication rounds\n",
    "num_rounds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sets all experiments\n",
    "def run_experiments(data, client_counts, federated_model_params, local_model_params, num_rounds, proportions, flag_profiler=False, flag_test_clients=False):\n",
    "   \n",
    "    experiments = [] # Experiment metrics\n",
    "    results_dir = \".Results\"  # Directory to save the results\n",
    "    os.makedirs(results_dir, exist_ok = True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    for num_clients in client_counts:\n",
    "        for proportion in proportions:\n",
    "\n",
    "            # Print all experiment settings\n",
    "            print(f'\\n *** Experiment details ***:\\n' \n",
    "            f'  - Number of Clients: {num_clients}\\n' \n",
    "            f'  - Number of Rounds: {num_rounds}\\n'\n",
    "            f'  - Number of Clusters: {federated_model_params[\"c_max\"]}\\n'\n",
    "            f'  - Data dristribution proportions: {proportion}\\n'\n",
    "            f'  - num_sigma Parameter: {federated_model_params[\"num_sigma\"]}\\n'\n",
    "            f'  - kappa_join Parameter: {federated_model_params[\"kappa_join\"]}\\n'\n",
    "            f'  - N_r Parameter: {federated_model_params[\"N_r\"]}')\n",
    "   \n",
    "            # Create train and test dataset\n",
    "            X = data.iloc[:, :-1].values\n",
    "            y = data.iloc[:, -1].values\n",
    "            client_raw_train, test_data, all_data = prepare_dataset(X, y, num_clients)\n",
    "\n",
    "            # Set undersampling techniques\n",
    "            balance_techniques = ['random'] * int(proportion)\n",
    "\n",
    "            if flag_profiler:\n",
    "\n",
    "                # Import profiling packages only if profiler is True\n",
    "                import cProfile\n",
    "                import yappi\n",
    "                try:\n",
    "                    # Try to load the memory_profiler extension if it's available\n",
    "                    get_ipython().run_line_magic('load_ext', 'memory_profiler')\n",
    "                except:\n",
    "                    print(\"Memory profiler not available.\")\n",
    "\n",
    "                print(f\"... with profiler\")\n",
    "                pr = cProfile.Profile()\n",
    "                pr.enable()\n",
    "                yappi.start()\n",
    "                \n",
    "                #Run one experiment with profiller\n",
    "                metrics = run_individual_experiment(federated_model_params=federated_model_params, \n",
    "                                                    local_model_params=local_model_params, \n",
    "                                                    num_clients=num_clients, num_rounds=num_rounds, \n",
    "                                                    client_raw_train=client_raw_train, test_data=test_data,\n",
    "                                                    balance_techniques=balance_techniques, test_clients=flag_test_clients)\n",
    "                \n",
    "                yappi.stop()\n",
    "                pr.disable()\n",
    "\n",
    "                pr.print_stats(sort='cumtime')\n",
    "                yappi.get_thread_stats().print_all()\n",
    "                yappi.get_func_stats().print_all()\n",
    "\n",
    "            else:\n",
    "\n",
    "                #Run one experiment\n",
    "                metrics = run_individual_experiment(federated_model_params=federated_model_params, \n",
    "                                                    local_model_params=local_model_params, \n",
    "                                                        num_clients=num_clients, num_rounds=num_rounds, \n",
    "                                                    client_raw_train=client_raw_train, test_data=test_data,\n",
    "                                                    balance_techniques=balance_techniques, test_clients=flag_test_clients)\n",
    "                \n",
    "            #Construct a specific name for the saved file\n",
    "            file_name = f'experiment_metrics_num_clients_{num_clients}_rounds_{num_rounds}_proportions_{proportion}_num_sigma_{federated_model_params[\"num_sigma\"]}_kappa_join_{int(federated_model_params[\"kappa_join\"]*10)}_N_r_{federated_model_params[\"N_r\"]}_clusters_{federated_model_params[\"c_max\"]}_Fscore_{1000*metrics[-1][\"federated_model\"][\"binary\"][\"f1_score\"]:.0f}.pth'\n",
    "            file_path = os.path.join(results_dir, file_name)\n",
    "\n",
    "            #Save the experiments\n",
    "            torch.save(metrics, file_path)\n",
    "            print(f\"Saved experiments to {file_path}\")\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracts the F1 score at the last communication round\n",
    "def evaluate_metric(experiments, round_number):\n",
    "    for experiment in experiments:\n",
    "        if experiment[1][-1]['round'] == int(round_number): #[1] is the experiment values, [0] is the name of the experiment, [-1] is the last round\n",
    "            return experiment[1][-1]['federated_model']['binary']['f1_score']\n",
    "    return None\n",
    "\n",
    "#Experiment with different model meta-parameters\n",
    "def run_parameterized_experiments(data, client_counts, proportions, num_rounds):\n",
    "    best_setting = None\n",
    "    best_f1_score = -float('inf')\n",
    "    all_experiments_metrics = []\n",
    "\n",
    "    for num_sigma in num_sigma_values:\n",
    "        for kappa_join in kappa_join_values:\n",
    "            for N_r in N_r_values:\n",
    "\n",
    "                    # Update the model parameters\n",
    "                    federated_model_params.update({\"num_sigma\": num_sigma, \"kappa_join\": kappa_join, \"N_r\": N_r})\n",
    "                    local_model_params.update({\"num_sigma\": num_sigma, \"kappa_join\": kappa_join, \"N_r\": N_r})\n",
    "\n",
    "                    # Run the experiment\n",
    "                    experiments = run_experiments(\n",
    "                        data=data,\n",
    "                        client_counts=client_counts,\n",
    "                        federated_model_params=federated_model_params, \n",
    "                        local_model_params=local_model_params, \n",
    "                        num_rounds=num_rounds, \n",
    "                        proportions=proportions, \n",
    "                        flag_profiler= False,\n",
    "                        flag_test_clients = False)\n",
    "\n",
    "                    # Store all experiments' metrics\n",
    "                    all_experiments_metrics.append(experiments)\n",
    "\n",
    "                    # Evaluate the F1 score of the 5th round\n",
    "                    f1_score = evaluate_metric(experiments, num_rounds)\n",
    "                    \n",
    "                    # Update the best setting if current setting's F1 score is better\n",
    "                    if f1_score and f1_score > best_f1_score:\n",
    "                        best_f1_score = f1_score\n",
    "                        best_setting = (num_sigma, kappa_join, N_r, proportions)\n",
    "\n",
    "    return best_setting, best_f1_score, all_experiments_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment to determine the best settings \n",
    "if flag_settings_experiment:\n",
    "    \n",
    "    # Run the experiments to determine the best settings\n",
    "    best_setting, best_f1_score, all_experiments_metrics = run_parameterized_experiments(data, client_counts, data_config_indices, num_rounds)\n",
    "    print(\"Best Setting:\", best_setting)\n",
    "    print(\"Best F1 Score:\", best_f1_score)\n",
    "\n",
    "    # Save the best setting and all experiments' metrics\n",
    "    torch.save({\n",
    "        \"best_setting\": best_setting,\n",
    "        \"best_f1_score\": best_f1_score,\n",
    "        \"experiments_metrics\": all_experiments_metrics\n",
    "    }, 'experiment_results.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *** Experiment details ***:\n",
      "  - Number of Clients: 3\n",
      "  - Number of Rounds: 31\n",
      "  - Number of Clusters: 300\n",
      "  - Data dristribution proportions: 5\n",
      "  - num_sigma Parameter: 15\n",
      "  - kappa_join Parameter: 0.4\n",
      "  - N_r Parameter: 30\n",
      "--- Communication Round 1 ---\n",
      "Client 1: {0: 645, 1: 129}\n",
      "Client 2: {0: 645, 1: 129}\n",
      "Client 3: {0: 640, 1: 128}\n",
      "Test Set: {0: 56856, 1: 106}\n",
      "\n",
      "Combined Number of Samples per Class:\n",
      "Class 0: 58786 samples\n",
      "Class 1: 492 samples\n",
      "\n",
      "Total Number of Samples Across All Datasets: 59278\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n",
      "Number of local model clusters = 282\n",
      "Updating agreggated model with client 1\n",
      "Updated var_glo values: tensor(14.7428)\n",
      "Federated clusters after merging = 92\n",
      "Number of local model clusters = 311\n",
      "Updating agreggated model with client 2\n",
      "Updated var_glo values: tensor(14.9009)\n",
      "Federated clusters after merging = 192\n",
      "Number of local model clusters = 324\n",
      "Updating agreggated model with client 3\n",
      "Updated var_glo values: tensor(14.4778)\n",
      "Federated clusters after merging = 283\n",
      "\n",
      "\n",
      "Evolving has been disabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m     proportion \u001b[38;5;241m=\u001b[39m best_setting[\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Run the experiments with the best setting for 30 rounds\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m experiments \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclient_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfederated_model_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfederated_model_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_model_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_model_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproportions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproportions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflag_profiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflag_test_clients\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 62\u001b[0m, in \u001b[0;36mrun_experiments\u001b[1;34m(data, client_counts, federated_model_params, local_model_params, num_rounds, proportions, flag_profiler, flag_test_clients)\u001b[0m\n\u001b[0;32m     57\u001b[0m     yappi\u001b[38;5;241m.\u001b[39mget_func_stats()\u001b[38;5;241m.\u001b[39mprint_all()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m#Run one experiment\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun_individual_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfederated_model_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfederated_model_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mlocal_model_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_model_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_clients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mclient_raw_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_raw_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mbalance_techniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbalance_techniques\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflag_test_clients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m#Construct a specific name for the saved file\u001b[39;00m\n\u001b[0;32m     69\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_metrics_num_clients_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_clients\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_rounds_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_rounds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_proportions_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproportion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_num_sigma_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfederated_model_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_sigma\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_kappa_join_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(federated_model_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkappa_join\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_N_r_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfederated_model_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN_r\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_clusters_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfederated_model_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_max\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Fscore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;241m*\u001b[39mmetrics[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfederated_model\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 51\u001b[0m, in \u001b[0;36mrun_individual_experiment\u001b[1;34m(federated_model_params, local_model_params, num_clients, num_rounds, client_raw_train, test_data, balance_techniques, test_clients)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Evaluate federated model\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m fed_scores, fed_pred, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfederated_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m fed_binary \u001b[38;5;241m=\u001b[39m calculate_metrics(fed_pred, test_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m fed_roc_auc \u001b[38;5;241m=\u001b[39m calculate_roc_auc(fed_scores, test_data)\n",
      "File \u001b[1;32mc:\\Users\\Miha\\OneDrive - Univerza v Ljubljani (1)\\eFedGauss\\eFedGauss\\utils\\utils_train.py:78\u001b[0m, in \u001b[0;36mtest_model_in_batches\u001b[1;34m(model, dataset, batch_size)\u001b[0m\n\u001b[0;32m     75\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Forward pass for the current batch\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m all_scores, pred_max, clusters \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Store the results\u001b[39;00m\n\u001b[0;32m     81\u001b[0m all_scores_list\u001b[38;5;241m.\u001b[39mappend(all_scores\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[1;32mc:\\Users\\Miha\\OneDrive - Univerza v Ljubljani (1)\\eFedGauss\\eFedGauss\\model\\eFedGauss.py:115\u001b[0m, in \u001b[0;36meFedGauss.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m    112\u001b[0m     \n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m#Compute the activations of the cluster membership functions\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatching_clusters \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc)\u001b[38;5;241m.\u001b[39mrepeat(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#Used to select clusters during training, here we select all\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmathematician\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_batched_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Defuzzify label scores for the entire batch\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     label_scores, preds_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsequence\u001b[38;5;241m.\u001b[39mdefuzzify_batch()\n",
      "File \u001b[1;32mc:\\Users\\Miha\\OneDrive - Univerza v Ljubljani (1)\\eFedGauss\\eFedGauss\\model\\math_operations.py:91\u001b[0m, in \u001b[0;36mMathOps.compute_batched_activation\u001b[1;34m(self, Z)\u001b[0m\n\u001b[0;32m     88\u001b[0m diff \u001b[38;5;241m=\u001b[39m (Z_expanded[:, :, :] \u001b[38;5;241m-\u001b[39m mu[:])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Perform batch matrix multiplication\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m d2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_inv_expanded\u001b[49m\u001b[43m)\u001b[49m, diff\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m#Initialize full_Gamma tensor\u001b[39;00m\n\u001b[0;32m     94\u001b[0m full_Gamma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(d2)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# List of client counts and data configuration indices\n",
    "client_counts = [3, 10]\n",
    "\n",
    "# Number of communication rounds\n",
    "num_rounds = 31\n",
    "\n",
    "# Data proportion for the experiments\n",
    "proportions = [5, 5, 5, 10, 10, 10, 1, 1, 1]\n",
    "\n",
    "# Model parameters\n",
    "local_model_params = {\n",
    "    \"feature_dim\": feature_dim,\n",
    "    \"num_classes\": 2,\n",
    "    \"kappa_n\": 1,\n",
    "    \"num_sigma\": 15,\n",
    "    \"kappa_join\": 0.4,\n",
    "    \"S_0\": 1e-10,\n",
    "    \"N_r\": 30,\n",
    "    \"c_max\": 1000,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "federated_model_params = {\n",
    "    \"feature_dim\": feature_dim,\n",
    "    \"num_classes\": 2,\n",
    "    \"kappa_n\": 1,\n",
    "    \"num_sigma\": 15,\n",
    "    \"kappa_join\": 0.4,\n",
    "    \"S_0\": 1e-10,\n",
    "    \"N_r\": 30,\n",
    "    \"c_max\": 300,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "if flag_settings_experiment:\n",
    "    # Update the model parameters with the best setting\n",
    "    federated_model_params.update({\n",
    "        \"num_sigma\": best_setting[0],\n",
    "        \"kappa_join\": best_setting[1],\n",
    "        \"N_r\": best_setting[2]\n",
    "    })\n",
    "\n",
    "    local_model_params.update({\n",
    "        \"num_sigma\": best_setting[0],\n",
    "        \"kappa_join\": best_setting[1],\n",
    "        \"N_r\": best_setting[2]\n",
    "    })\n",
    "\n",
    "    # Set the number of rounds and proportion with the best setting\n",
    "    proportion = best_setting[3]\n",
    "\n",
    "# Run the experiments with the best setting for 30 rounds\n",
    "experiments = run_experiments(\n",
    "            data=data, \n",
    "            client_counts=client_counts,\n",
    "            federated_model_params=federated_model_params, \n",
    "            local_model_params=local_model_params, \n",
    "            num_rounds=num_rounds, \n",
    "            proportions=proportions, \n",
    "            flag_profiler=False,\n",
    "            flag_test_clients = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select experiment for plotting\n",
    "name = \"experiment_metrics_num_clients_3_rounds_31_proportions_10_num_sigma_15_kappa_join_4_N_r_30_clusters_1000_Fscore_64.pth\"\n",
    "experiments = torch.load(f\".Results/{name}\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show experiment metrics over the communication rounds\n",
    "figs_metrics = []\n",
    "figs_clusters = []\n",
    "experiment_name = []\n",
    "rounds_max = 30\n",
    "if len(name) > 40:\n",
    "        metrics = experiments\n",
    "        rounds = [m['round'] for m in metrics]\n",
    "        if rounds[-1] > rounds_max:\n",
    "             rounds = rounds[:rounds_max]\n",
    "             metrics = metrics[:rounds_max]\n",
    "        # Plot and collect figures\n",
    "       \n",
    "        figs_metrics.append(plot_metric_data(metrics, ['f1_score', 'precision', 'recall'], rounds, ''))\n",
    "        figs_clusters.append(plot_cluster_data(metrics, rounds))\n",
    "        print(f\"{100*metrics[-1]['federated_model']['binary']['f1_score']:.0f}\")\n",
    "else:\n",
    "        rounds = [m['round'] for m in experiments]\n",
    "        if rounds[-1] > rounds_max:\n",
    "             rounds = rounds[:rounds_max]\n",
    "             experiments = experiments[:rounds_max]\n",
    "\n",
    "        # Plot and collect figures\n",
    "        figs_metrics.append(plot_metric_data(experiments, ['f1_score', 'precision', 'recall'], rounds,\"\", legend=experiments))\n",
    "        figs_clusters.append(plot_cluster_data(experiments, rounds, legend=experiments))\n",
    "        print(f\"{100*experiments[-1]['federated_model']['binary']['f1_score']:.0f}\")\n",
    "\n",
    "# Save figures from fig_metrics\n",
    "for i, figure in enumerate(figs_metrics):\n",
    "    save_path = f\".Images/credit_fraud_metrics_{name}.pdf\"\n",
    "    save_figure(figure, save_path, \"pdf\")\n",
    "\n",
    "# Save figures from fig_clusters\n",
    "for i, figure in enumerate(figs_clusters):\n",
    "    save_path = f\".Images/credit_fraud_clusters_{name}.pdf\"\n",
    "    save_figure(figure, save_path, \"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract settings from filename\n",
    "def extract_settings(filename):\n",
    "    pattern = r\"(num_clients|rounds|proportions|num_sigma|kappa_join|N_r|clusters)_([0-9]+)\"\n",
    "    matches = re.findall(pattern, filename)\n",
    "    return {match[0]: int(match[1]) for match in matches}\n",
    "\n",
    "# Load all experiments and group by settings\n",
    "experiments_by_settings = {}\n",
    "directory = \".Results\"\n",
    "round_of_interest = 30  # Specify the round of interest\n",
    "num_experiments = 3  # Specify the number of experiments required for each setting\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith(\"experiment_metrics_\"):\n",
    "        settings = extract_settings(filename)\n",
    "        settings_key = tuple(sorted(settings.items()))\n",
    "\n",
    "        if settings_key not in experiments_by_settings:\n",
    "            experiments_by_settings[settings_key] = []\n",
    "\n",
    "        experiment_data = torch.load(f\"{directory}/{filename}\")\n",
    "        experiments_by_settings[settings_key].append(experiment_data)\n",
    "\n",
    "# Calculate average and std of metrics at the specified round for each settings group\n",
    "metrics_by_settings = {}\n",
    "\n",
    "for settings, experiments in experiments_by_settings.items():\n",
    "    if len(experiments) >= num_experiments:\n",
    "        f1_scores, precisions, recalls, roc_aucs = [], [], [], []\n",
    "\n",
    "        for experiment in experiments:\n",
    "            if len(experiment) >= round_of_interest:\n",
    "                round_data = experiment[round_of_interest - 1]['federated_model']\n",
    "                f1_scores.append(100*round_data['binary']['f1_score'])\n",
    "                precisions.append(100*round_data['binary']['precision'])\n",
    "                recalls.append(100*round_data['binary']['recall'])\n",
    "                roc_aucs.append(round_data['roc_auc'])\n",
    "\n",
    "        metrics_by_settings[settings] = {\n",
    "            'avg_f1': np.mean(f1_scores), 'std_f1': np.std(f1_scores),\n",
    "            'avg_precision': np.mean(precisions), 'std_precision': np.std(precisions),\n",
    "            'avg_recall': np.mean(recalls), 'std_recall': np.std(recalls),\n",
    "            'avg_roc_auc': np.mean(roc_aucs), 'std_roc_auc': np.std(roc_aucs)\n",
    "        }\n",
    "\n",
    "for settings, metrics in metrics_by_settings.items():\n",
    "    settings_str = ', '.join(f\"{key}={value}\" for key, value in settings)\n",
    "    print(f\"Settings: {settings_str} -> Average F1 Score: {metrics['avg_f1']:.4f}, Standard Deviation: {metrics['std_f1']:.4f}, \"\n",
    "          f\"Average Precision: {metrics['avg_precision']:.4f}, Standard Deviation: {metrics['std_precision']:.4f}, \"\n",
    "          f\"Average Recall: {metrics['avg_recall']:.4f}, Standard Deviation: {metrics['std_recall']:.4f}, \"\n",
    "          f\"Average ROC AUC: {metrics['avg_roc_auc']:.4f}, Standard Deviation: {metrics['std_roc_auc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(data):\n",
    "    latex_table = \"\\\\begin{table}[!t]\\n\"\n",
    "    latex_table += \"\\\\centering\\n\"\n",
    "    latex_table += \"\\\\setlength{\\\\tabcolsep}{4pt}\\n\"  # Changed tabcolsep to 5pt\n",
    "    latex_table += \"\\\\scriptsize\\n\"\n",
    "    latex_table += \"\\\\caption{Results of the Credit Card Fraud Detection}\\n\"\n",
    "    latex_table += \"\\\\begin{tabular}{ll|cccccccc}\\n\"\n",
    "    latex_table += \"\\\\toprule\\n\"\n",
    "    # Changed header row to use bold font and mathematical symbols where necessary\n",
    "    latex_table += \"\\\\bf{Data} & $\\\\mathbf{\\\\kappa}_c$ & \\\\multicolumn{2}{c}{\\\\bf{Precision}(\\\\%)$\\\\uparrow$} & \\\\multicolumn{2}{c}{\\\\bf{Recall}(\\\\%)$\\\\uparrow$} & \\\\multicolumn{2}{c}{\\\\bf{F1 score}(\\\\%)$\\\\uparrow$} & \\\\multicolumn{2}{c}{\\\\bf{ROC AUC}$\\\\uparrow$} \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\cmidrule(lr){3-4} \\\\cmidrule(lr){5-6} \\\\cmidrule(lr){7-8} \\\\cmidrule(lr){9-10}\\n\"\n",
    "    # Changed number headings to bold\n",
    "    latex_table += \"&  & \\\\bf{3} & \\\\bf{10} & \\\\bf{3} & \\\\bf{10} & \\\\bf{3} & \\\\bf{10} & \\\\bf{3} & \\\\bf{10} \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\midrule\\n\"\n",
    "\n",
    "    # Find the best F1 score for 3 clients and 10 clients\n",
    "    best_f1_3 = max(metrics['avg_f1'] for settings, metrics in data.items() if 'avg_f1' in metrics and dict(settings).get('num_clients') == 3)\n",
    "    best_f1_10 = max(metrics['avg_f1'] for settings, metrics in data.items() if 'avg_f1' in metrics and dict(settings).get('num_clients') == 10)\n",
    "\n",
    "\n",
    "    # Sort and group data by settings, excluding num_clients\n",
    "    grouped_data = {}\n",
    "    for settings, metrics in data.items():\n",
    "        # Create a key for grouping, excluding 'num_clients'\n",
    "        group_key = tuple((k, v) for k, v in settings if k != 'num_clients')\n",
    "        if group_key not in grouped_data:\n",
    "            grouped_data[group_key] = {}\n",
    "        num_clients = dict(settings).get('num_clients')\n",
    "        grouped_data[group_key][num_clients] = metrics\n",
    "\n",
    "    # Sort grouped data by proportions and then by clusters\n",
    "    sorted_grouped_data = sorted(grouped_data.items(), key=lambda x: (dict(x[0]).get('proportions', 0), dict(x[0]).get('clusters', 0)))\n",
    "   \n",
    "    last_proportion = None\n",
    "    for settings, client_data in sorted_grouped_data:\n",
    "        proportions = dict(settings).get('proportions', 'N/A')\n",
    "        \n",
    "        # Add a horizontal line when the proportion setting changes\n",
    "        if last_proportion is not None and last_proportion != proportions:\n",
    "            latex_table += \"\\\\hline\\n\"\n",
    "        last_proportion = proportions\n",
    "        metrics_3_clients = client_data.get(3)\n",
    "        metrics_10_clients =  client_data.get(10)\n",
    "\n",
    "        # Function to format metric value or empty placeholder\n",
    "        format_metric = lambda val: f\"$ {val:.1f} $\" if val is not None else \"$ $\"\n",
    "        format_roc_auc = lambda val: f\"$ {val:.3f} $\" if val is not None else \"$ $\"\n",
    "        \n",
    "        # Extract and format the dataset value\n",
    "        proportions = dict(settings).get('proportions', 'N/A')\n",
    "        dataset = f\"{proportions}:1\" if proportions != 'N/A' else \"N/A\"\n",
    "        clusters = 2*dict(settings).get('clusters', 'N/A')\n",
    "\n",
    "\n",
    "        # Extract and format metrics for 3 clients\n",
    "        if metrics_3_clients:\n",
    "            avg_precision_3 = format_metric(metrics_3_clients['avg_precision'])\n",
    "            avg_recall_3 = format_metric(metrics_3_clients['avg_recall'])\n",
    "            avg_f1_3 = format_metric(metrics_3_clients['avg_f1'])\n",
    "            avg_roc_auc_3 = format_roc_auc(metrics_3_clients['avg_roc_auc'])\n",
    "        else:\n",
    "            avg_precision_3 = avg_recall_3 = avg_f1_3 = avg_roc_auc_3 = \"$ $\"\n",
    "\n",
    "        # Extract and format metrics for 10 clients\n",
    "        if metrics_10_clients:\n",
    "            avg_precision_10 = format_metric(metrics_10_clients['avg_precision'])\n",
    "            avg_recall_10 = format_metric(metrics_10_clients['avg_recall'])\n",
    "            avg_f1_10 = format_metric(metrics_10_clients['avg_f1'])\n",
    "            avg_roc_auc_10 = format_roc_auc(metrics_10_clients['avg_roc_auc'])\n",
    "        else:\n",
    "            avg_precision_10 = avg_recall_10 = avg_f1_10 = avg_roc_auc_10 = \"$ $\"\n",
    "            \n",
    "        # Bold the best F1 scores\n",
    "        avg_f1_3 = f\"$\\\\textbf{{{metrics_3_clients['avg_f1']:.1f}}}$\" if metrics_3_clients and metrics_3_clients['avg_f1'] == best_f1_3 else f\"${metrics_3_clients['avg_f1']:.1f}$\" if metrics_3_clients else \"$ $\"\n",
    "        avg_f1_10 = f\"$\\\\textbf{{{metrics_10_clients['avg_f1']:.1f}}}$\" if metrics_10_clients and metrics_10_clients['avg_f1'] == best_f1_10 else f\"${metrics_10_clients['avg_f1']:.1f}$\" if metrics_10_clients else \"$ $\"\n",
    "\n",
    "        latex_table += f\"    {dataset} & {clusters} & {avg_precision_3} & {avg_precision_10} & {avg_recall_3} & {avg_recall_10} & {avg_f1_3} & {avg_f1_10} & {avg_roc_auc_3} & {avg_roc_auc_10}\\\\\\\\\\n\"\n",
    "\n",
    "    latex_table += \"\\\\bottomrule\\n\"\n",
    "    latex_table += \"\\\\multicolumn{10}{p{0.95\\linewidth}}{\\\\tiny The first column (Data) shown the proportion of non-fraud to fraud samples in the training data, which was selected by random undersampling. The second column ($\\kappa_c$) shows the maximum number of clusters of the federated model, before pruning was used. The evaluation metrics shown are averages for 3 repetitions.}\"\n",
    "    latex_table += \"\\\\end{tabular}\\n\"\n",
    "    latex_table += \"\\\\label{tab:credit_fraud}\\n\"\n",
    "    latex_table += \"\\\\end{table}\\n\"\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "print(generate_latex_table(metrics_by_settings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write the README content to a Python file as a string, preserving the formatting\n",
    "\n",
    "readme_content = \"\"\"\n",
    "# eFedGauss: A Federated Approach to Fuzzy Multivariate Gaussian Clustering\n",
    "**This paper is in the review process.**\n",
    "\n",
    "## Overview\n",
    "eFedGauss introduces a novel method for data clustering and classification in federated learning environments. ...\n",
    "\n",
    "## Repository Structure\n",
    "\n",
    "The eFedGauss project is organized as follows:\n",
    "\n",
    "- `model/`: Contains the implementation of the eFedGauss algorithm and its associated operations.\n",
    "  - `clustering_operations/`: Functions related to the clustering process.\n",
    "  - `consequence_operations/`: Operations for handling the consequence output of the model.\n",
    "  - `eFedGauss/`: The main eFedGauss algorithm implementation.\n",
    "  - `federated_operations/`: Federated learning specific operations.\n",
    "  - `math_operations/`: Mathematical functions used across the model (distance computation).\n",
    "  - `merging_mechanism/`: Logic for merging clusters.\n",
    "  - `model_operations/`: Core operations for managing the lifecycle of the clustering model.\n",
    "  - `removal_mechanism/`: Methods for removing clusters.\n",
    "\n",
    "- `utils/`: Utility scripts to support the main algorithm.\n",
    "  - `utils_dataset/`: Utilities for data handling and preprocessing.\n",
    "  - `utils_metrics/`: Metrics for evaluating the model's performance.\n",
    "  - `utils_plots/`: Plotting functions to visualize the results.\n",
    "  - `utils_tables/`: Functions to generate result tables.\n",
    "  - `utils_train/`: Training and testing.\n",
    "\n",
    "- `credit_card_fraud_experiment/`: Experimental setup and results for the credit card fraud detection.\n",
    "\n",
    "- `iris_flower_experiment/`: Experimental setup and results for the Iris Flower classification.\n",
    "\n",
    "- `synthetic_experiment/`: Experimental setup and results for testing with synthetic data.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To get started with eFedGauss, follow the setup instructions below.\n",
    "\n",
    "### Setting Up the Environment\n",
    "\n",
    "**Create and activate a Conda environment using the following commands:**\n",
    "\n",
    "\\```bash\n",
    "conda create -n eFedGauss python=3.10\n",
    "conda activate eFedGauss\n",
    "\\```\n",
    "\n",
    "### Installing PyTorch and Related Packages\n",
    "\n",
    "**Install PyTorch and related packages with the following command:**\n",
    "\n",
    "\\```bash\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\\```\n",
    "\n",
    "### Installing Additional Requirements\n",
    "\n",
    "**Install the remaining requirements from the `requirements.txt` file:**\n",
    "\n",
    "\\```bash\n",
    "pip install -r requirements.txt\n",
    "\\```\n",
    "\n",
    "## Usage\n",
    "\n",
    "**Run the experiments using the Jupyter notebooks provided:**\n",
    "\n",
    "- `credit_card_fraud_experiment.ipynb`: For credit card fraud detection.\n",
    "- `iris_flower_experiment.ipynb`: For Iris Flower classification.\n",
    "- `synthetic_experiment.ipynb`: For synthetic data experiments.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The required packages are listed in the `requirements.txt` file.\n",
    "\n",
    "## Contribution\n",
    "\n",
    "Contributions are welcome. Please follow the standard fork-and-pull request workflow.\n",
    "\n",
    "## License\n",
    "\n",
    "eFedGauss is under GNU General Public License v3 (GPLv3). See `LICENSE` for more details.\n",
    "\"\"\"\n",
    "\n",
    "# Save to a .py file\n",
    "file_path = 'README.md'\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(readme_content)\n",
    "\n",
    "file_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
